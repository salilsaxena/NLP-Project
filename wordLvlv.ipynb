{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets start by reading the data, and tokenizing it by the help of `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/salil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # to enable tokenizer of nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"shakespeare.txt\"\n",
    "raw_data = open(dataset_name, \"r\").read().lower() # lets make the whole text to lower case as it will not differenciate b/w The and the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of letters: 4194411\n",
      "Total number of words: 691273\n",
      "Total number of letter in these each words: 3503139\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of letters:\", len(raw_data))\n",
    "print(\"Total number of words:\",len(raw_data.split(' ')))\n",
    "print(\"Total number of letter in these each words:\",end = '')\n",
    "s = 0\n",
    "for i in raw_data.split(' '):\n",
    "    s+=len(i)\n",
    "print('',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initially tokens variable is: <class 'list'> lets convert it into numpy array\n",
      "#Tokens: 932696\n",
      "#Unique tokens: 53433\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(raw_data)\n",
    "print(\"Initially tokens variable is:\",type(tokens),\"lets convert it into numpy array\")\n",
    "tokens = np.asarray(tokens)\n",
    "uniqTokens = np.unique(tokens)\n",
    "print(\"#Tokens:\",len(tokens))\n",
    "print(\"#Unique tokens:\",len(uniqTokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the corporus task is complete. \n",
    "# Now we have to encode the taken tokens in lexographic order, we have below 2 ways: \n",
    "- One-hot encoding\n",
    "- Word embedding\n",
    "## One-hot encdoding: \n",
    "Each word will be represented by a vector of dimensions: `53433`(*no of unique tokens*), and encoding 932696 tokens into such vector will be memory heavy (**932696\\*53433 integers: which roughly equal to 185GB, so we wont do that. As I cant offord 185GB of RAM and I am not in a mood to process and save each of those tokens one-by-one**)\n",
    "## Word embedding\n",
    "Each token will be encoded into maybe 100D vector so max-to-max *88MB* that is way lesser than "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
